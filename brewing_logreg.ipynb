{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "description": "Use Caffe as a generic SGD optimizer to train logistic regression on non-image HDF5 data.",
    "example_name": "Off-the-shelf SGD for classification",
    "include_in_docs": true,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "priority": 4,
    "colab": {
      "name": "brewing-logreg.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TimeIsTheChoice/ColabCaffe/blob/master/brewing_logreg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJAp6ubI_bU5",
        "colab_type": "text"
      },
      "source": [
        "## Caffe Prerpare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOMT3S1AHYe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Check Cuda_Tools_Version {display-mode: \"form\"}\n",
        "!nvidia-smi\n",
        "!nvcc -V\n",
        "# This code will be hidden when the notebook is loaded.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkkPjwgeGwXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Check cuda tools version {display-mode: \"form\"}\n",
        "If_not_10_0_130_click_box = False #@param {type:\"boolean\"}\n",
        "if If_not_10_0_130_click_box:\n",
        "  !apt  --purge remove \"cublas*\" \"cuda*\"\n",
        "  !apt  --purge remove \"nvidia*\" \n",
        "  !wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
        "  !apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub && sudo apt update\n",
        "  !dpkg -i cuda-repo-ubuntu1804_10.0.130-1_amd64.deb\n",
        "  #!apt update\n",
        "  !apt install -y cuda=10.0.130-1\n",
        "\n",
        "  # Install CuDNN 7 and NCCL 2\n",
        "  !wget https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
        "  !dpkg -i nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb\n",
        "\n",
        "  #!apt update\n",
        "  !apt install -y libcudnn7 libcudnn7-dev libnccl2 libc-ares-dev\n",
        "\n",
        "  !apt autoremove\n",
        "  #!apt upgrade\n",
        "\n",
        "  # Link libraries to standard locations\n",
        "  !mkdir -p /usr/local/cuda-10.0/nccl/lib\n",
        "  !ln -s /usr/lib/x86_64-linux-gnu/libnccl.so.2 /usr/local/cuda/nccl/lib/\n",
        "  !ln -s /usr/lib/x86_64-linux-gnu/libcudnn.so.7 /usr/local/cuda-10.0/lib64/  \n",
        "else:\n",
        "  print(\"version correct,goning on!\")\n",
        "# This code will be hidden when the notebook is loaded.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUkanxI0F_8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title clone caffe and install dependence {display-mode: \"form\"}\n",
        "dirname = os.path.abspath('/content/caffe')\n",
        "if not os.path.exists(dirname):\n",
        "  !apt install python3-opencv\n",
        "  !apt-get install libopenblas-dev # OpenBLAS\n",
        "  # Other dependencies\n",
        "  !apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler\n",
        "  !apt-get install — no-install-recommends libboost-all-dev\n",
        "  !apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev\n",
        "  !pip3 install protobuf\n",
        "  !apt-get install the python3-dev\n",
        "\n",
        "  import os\n",
        "  os.chdir('/content')\n",
        "  !rm -rf caffe\n",
        "  !git clone https://github.com/TimeIsTheChoice/caffe.git\n",
        "  !cd caffe\n",
        "else:\n",
        "  print(\"Caffe exist!\")\n",
        "\n",
        "# This code will be hidden when the notebook is loaded.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSiN4pgFBfdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Get cpu cores number {display-mode: \"form\"}\n",
        "!nproc --all\n",
        "!echo \"Threads/core: $(nproc --all)\"\n",
        "# This code will be hidden when the notebook is loaded.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8rC2S1WBubX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title If build faild,click me! {display-mode: \"form\"}\n",
        "Build_Failed = False #@param {type:\"boolean\"}\n",
        "if Build_Failed:\n",
        "  !make clean\n",
        "else:\n",
        "  print(\"nothing happen!\")\n",
        "# This code will be hidden when the notebook is loaded.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EPJLybcBUwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Build Caffe {display-mode: \"form\"}\n",
        "Build_Caffe = False #@param {type:\"boolean\"}\n",
        "if Build_Caffe:\n",
        "  import os\n",
        "  os.chdir('/content/caffe')\n",
        "  !make all -j$(nproc --all) # 4 represents number of CPU Cores\n",
        "  !make pycaffe -j$(nproc --all) # 4 represents number of CPU Cores\n",
        "  !export PYTHONPATH=~/caffe/python:$PYTHONPATH\n",
        "else:\n",
        "  print(\"Make sure you want build it,please!\")\n",
        "# This code will be hidden when the notebook is loaded.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hxvT1kwBDrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title check Caffe  {display-mode: \"form\"}\n",
        "caffe_root = '/content/caffe/'  # this file should be run from {caffe_root}/examples (otherwise change this line)\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, caffe_root + 'python')\n",
        "import caffe\n",
        "print(\"import successful!\")\n",
        "# This code will be hidden when the notebook is loaded.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8dmcUDlAy7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Check runtime whetehr gpu or not {display-mode: \"form\"}\n",
        "#' ' means CPU whereas '/device:G:0' means GPU\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "# This code will be hidden when the notebook is loaded.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppUL6nwNAoNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title check GPU memory utilization {display-mode: \"form\"}\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() \n",
        "# This code will be hidden when the notebook is loaded.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9cOnNG7h-0J",
        "colab_type": "text"
      },
      "source": [
        "# Brewing Logistic Regression then Going Deeper\n",
        "\n",
        "While Caffe is made for deep networks it can likewise represent \"shallow\" models like logistic regression for classification. We'll do simple logistic regression on synthetic data that we'll generate and save to HDF5 to feed vectors to Caffe. Once that model is done, we'll add layers to improve accuracy. That's what Caffe is about: define a model, experiment, and then deploy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEt_k3lGh-0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "os.chdir('..')\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, './python')\n",
        "import caffe\n",
        "\n",
        "\n",
        "import os\n",
        "import h5py\n",
        "import shutil\n",
        "import tempfile\n",
        "\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import sklearn.linear_model\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsteR4vfh-0T",
        "colab_type": "text"
      },
      "source": [
        "Synthesize a dataset of 10,000 4-vectors for binary classification with 2 informative features and 2 noise features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJT0GK8Ch-0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = sklearn.datasets.make_classification(\n",
        "    n_samples=10000, n_features=4, n_redundant=0, n_informative=2, \n",
        "    n_clusters_per_class=2, hypercube=False, random_state=0\n",
        ")\n",
        "\n",
        "# Split into train and test\n",
        "X, Xt, y, yt = sklearn.model_selection.train_test_split(X, y)\n",
        "\n",
        "# Visualize sample of the data\n",
        "ind = np.random.permutation(X.shape[0])[:1000]\n",
        "df = pd.DataFrame(X[ind])\n",
        "_ = pd.plotting.scatter_matrix(df, figsize=(9, 9), diagonal='kde', marker='o', s=40, alpha=.4, c=y[ind])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RojhdxXt9Z9B",
        "colab_type": "text"
      },
      "source": [
        "## Train and Test\n",
        ">Learn and evaluate scikit-learn's logistic regression with stochastic gradient descent (SGD) training. Time and check the classifier's accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1IPbjYah-0a",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "alter\n",
        "```\n",
        "clf = sklearn.linear_model.SGDClassifier(\n",
        "    loss='log', n_iter =1000, penalty='l2', alpha=5e-4, class_weight='balanced')\n",
        "```\n",
        "to\n",
        "\n",
        "\n",
        "```\n",
        "clf = sklearn.linear_model.SGDClassifier(\n",
        "    loss='log', max_iter=1000, penalty='l2', alpha=5e-4, class_weight='balanced')\n",
        "```\n",
        "cite [How to fix TypeError: __init__() got an unexpected keyword argument 'max_iter'?](https://forums.databricks.com/questions/20232/how-to-fix-typeerror-init-got-an-unexpected-keywor.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUwHapjUh-0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%timeit\n",
        "# Train and test the scikit-learn SGD logistic regression.\n",
        "clf = sklearn.linear_model.SGDClassifier(\n",
        "    loss='log', max_iter=1000, penalty='l2', alpha=5e-4, class_weight='balanced')\n",
        "\n",
        "clf.fit(X, y)\n",
        "yt_pred = clf.predict(Xt)\n",
        "print('Accuracy: {:.3f}'.format(sklearn.metrics.accuracy_score(yt, yt_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I31eLNqoh-0f",
        "colab_type": "text"
      },
      "source": [
        "## Save the dataset to HDF5 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVQWsD9A-1HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Write out the data to HDF5 files in a temp directory.\n",
        "# This file is assumed to be caffe_root/examples/hdf5_classification.ipynb\n",
        "dirname = os.path.abspath('/content/caffe/examples/hdf5_classification/data')\n",
        "if not os.path.exists(dirname):\n",
        "    os.makedirs(dirname)\n",
        "\n",
        "train_filename = os.path.join(dirname, 'train.h5')\n",
        "test_filename = os.path.join(dirname, 'test.h5')\n",
        "\n",
        "# HDF5DataLayer source should be a file containing a list of HDF5 filenames.\n",
        "# To show this off, we'll list the same data file twice."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9prVpfHG-1b7",
        "colab_type": "text"
      },
      "source": [
        ">such as\n",
        "\n",
        "```\n",
        "/content/caffe/examples/hdf5_classification/data/train.h5\n",
        "/content/caffe/examples/hdf5_classification/data/train.h5\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYDd3-uSh-0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "with h5py.File(train_filename, 'w') as f:\n",
        "    f['data'] = X\n",
        "    f['label'] = y.astype(np.float32)\n",
        "with open(os.path.join(dirname, 'train.txt'), 'w') as f:\n",
        "    f.write(train_filename + '\\n')\n",
        "    f.write(train_filename + '\\n')\n",
        "    \n",
        "# HDF5 is pretty efficient, but can be further compressed.\n",
        "comp_kwargs = {'compression': 'gzip', 'compression_opts': 1}\n",
        "with h5py.File(test_filename, 'w') as f:\n",
        "    f.create_dataset('data', data=Xt, **comp_kwargs)\n",
        "    f.create_dataset('label', data=yt.astype(np.float32), **comp_kwargs)\n",
        "with open(os.path.join(dirname, 'test.txt'), 'w') as f:\n",
        "    f.write(test_filename + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2pnzRwR_mFD",
        "colab_type": "text"
      },
      "source": [
        "## logistics regression in Cafee "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-bnppTQh-0o",
        "colab_type": "text"
      },
      "source": [
        "Let's define logistic regression in Caffe through Python net specification. This is a quick and natural way to define nets that sidesteps manually editing the protobuf model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6dJhvht_7hU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title verify drectory {display-mode: \"form\"}\n",
        "import os\n",
        "os.chdir('/content/caffe')\n",
        "!pwd\n",
        "# This code will be hidden when the notebook is loaded.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry4rorLEh-0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from caffe import layers as L\n",
        "from caffe import params as P\n",
        "\n",
        "def logreg(hdf5, batch_size):\n",
        "    # logistic regression: data, matrix multiplication, and 2-class softmax loss\n",
        "    n = caffe.NetSpec()\n",
        "    n.data, n.label = L.HDF5Data(batch_size=batch_size, source=hdf5, ntop=2)\n",
        "    n.ip1 = L.InnerProduct(n.data, num_output=2, weight_filler=dict(type='xavier'))\n",
        "    n.accuracy = L.Accuracy(n.ip1, n.label)\n",
        "    n.loss = L.SoftmaxWithLoss(n.ip1, n.label)\n",
        "    return n.to_proto()\n",
        "\n",
        "train_net_path = 'examples/hdf5_classification/logreg_auto_train.prototxt'\n",
        "with open(train_net_path, 'w') as f:\n",
        "    f.write(str(logreg('examples/hdf5_classification/data/train.txt', 10)))\n",
        "\n",
        "test_net_path = 'examples/hdf5_classification/logreg_auto_test.prototxt'\n",
        "with open(test_net_path, 'w') as f:\n",
        "    f.write(str(logreg('examples/hdf5_classification/data/test.txt', 10)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjT6B3gADnzR",
        "colab_type": "text"
      },
      "source": [
        "### Define our \"solver\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rd5tvTjh-0s",
        "colab_type": "text"
      },
      "source": [
        "Now, we'll define our \"solver\" which trains the network by specifying the locations of the train and test nets we defined above, as well as setting values for various parameters used for learning, display, and \"snapshotting\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF3fJJgch-0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from caffe.proto import caffe_pb2\n",
        "\n",
        "def solver(train_net_path, test_net_path):\n",
        "    s = caffe_pb2.SolverParameter()\n",
        "\n",
        "    # Specify locations of the train and test networks.\n",
        "    s.train_net = train_net_path\n",
        "    s.test_net.append(test_net_path)\n",
        "\n",
        "    s.test_interval = 1000  # Test after every 1000 training iterations.\n",
        "    s.test_iter.append(250) # Test 250 \"batches\" each time we test.\n",
        "\n",
        "    s.max_iter = 1000000      # # of times to update the net (training iterations)\n",
        "\n",
        "    # Set the initial learning rate for stochastic gradient descent (SGD).\n",
        "    s.base_lr = 0.01        \n",
        "\n",
        "    # Set `lr_policy` to define how the learning rate changes during training.\n",
        "    # Here, we 'step' the learning rate by multiplying it by a factor `gamma`\n",
        "    # every `stepsize` iterations.\n",
        "    s.lr_policy = 'step'\n",
        "    s.gamma = 0.1\n",
        "    s.stepsize = 5000\n",
        "\n",
        "    # Set other optimization parameters. Setting a non-zero `momentum` takes a\n",
        "    # weighted average of the current gradient and previous gradients to make\n",
        "    # learning more stable. L2 weight decay regularizes learning, to help prevent\n",
        "    # the model from overfitting.\n",
        "    s.momentum = 0.9\n",
        "    s.weight_decay = 5e-4\n",
        "\n",
        "    # Display the current training loss and accuracy every 1000 iterations.\n",
        "    s.display = 1000\n",
        "\n",
        "    # Snapshots are files used to store networks we've trained.  Here, we'll\n",
        "    # snapshot every 10K iterations -- just once at the end of training.\n",
        "    # For larger networks that take longer to train, you may want to set\n",
        "    # snapshot < max_iter to save the network and training state to disk during\n",
        "    # optimization, preventing disaster in case of machine crashes, etc.\n",
        "    s.snapshot = 10000\n",
        "    s.snapshot_prefix = 'examples/hdf5_classification/data/train'\n",
        "\n",
        "    # We'll train on the CPU for fair benchmarking against scikit-learn.\n",
        "    # Changing to GPU should result in much faster training!\n",
        "    s.solver_mode = caffe_pb2.SolverParameter.CPU\n",
        "    \n",
        "    return s\n",
        "\n",
        "solver_path = 'examples/hdf5_classification/logreg_solver.prototxt'\n",
        "with open(solver_path, 'w') as f:\n",
        "    f.write(str(solver(train_net_path, test_net_path)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZOwl33GJ1yn",
        "colab_type": "text"
      },
      "source": [
        "### Run in python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ-DgyNkh-0y",
        "colab_type": "text"
      },
      "source": [
        "Time to learn and evaluate our Caffeinated logistic regression in Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RZk0fonExob",
        "colab_type": "text"
      },
      "source": [
        ">cpu is significantly faster in here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb-NNM3sh-00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%timeit\n",
        "caffe.set_mode_cpu()\n",
        "solver = caffe.get_solver(solver_path)\n",
        "solver.solve()\n",
        "\n",
        "accuracy = 0\n",
        "batch_size = solver.test_nets[0].blobs['data'].num\n",
        "test_iters = int(len(Xt) / batch_size)\n",
        "for i in range(test_iters):\n",
        "    solver.test_nets[0].forward()\n",
        "    accuracy += solver.test_nets[0].blobs['accuracy'].data\n",
        "accuracy /= test_iters\n",
        "\n",
        "print(\"Accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orLK402fKD_N",
        "colab_type": "text"
      },
      "source": [
        "### Run in command"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHL2eBuhh-05",
        "colab_type": "text"
      },
      "source": [
        "Do the same through the command line interface for detailed output on the model and solving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnuYHtUYh-06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!build/tools/caffe train -solver examples/hdf5_classification/logreg_solver.prototxt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "769D5nEIh-09",
        "colab_type": "text"
      },
      "source": [
        "If you look at output or the `logreg_auto_train.prototxt`, you'll see that the model is simple logistic regression.\n",
        "We can make it a little more advanced by introducing a non-linearity between weights that take the input and weights that give the output -- now we have a two-layer network.\n",
        "That network is given in `nonlinear_auto_train.prototxt`, and that's the only change made in `nonlinear_logreg_solver.prototxt` which we will now use.\n",
        "\n",
        "The final accuracy of the new network should be higher than logistic regression!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7vr3mPBh-0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from caffe import layers as L\n",
        "from caffe import params as P\n",
        "\n",
        "def nonlinear_net(hdf5, batch_size):\n",
        "    # one small nonlinearity, one leap for model kind\n",
        "    n = caffe.NetSpec()\n",
        "    n.data, n.label = L.HDF5Data(batch_size=batch_size, source=hdf5, ntop=2)\n",
        "    # define a hidden layer of dimension 40\n",
        "    n.ip1 = L.InnerProduct(n.data, num_output=40, weight_filler=dict(type='xavier'))\n",
        "    # transform the output through the ReLU (rectified linear) non-linearity\n",
        "    n.relu1 = L.ReLU(n.ip1, in_place=True)#By setting the bottom and the top blob to be the same we can tell Caffe to do \"in-place\",ReLU,Dropout,BatchNorm,Scale\n",
        "    # score the (now non-linear) features\n",
        "    n.ip2 = L.InnerProduct(n.ip1, num_output=2, weight_filler=dict(type='xavier'))\n",
        "    # same accuracy and loss as before\n",
        "    n.accuracy = L.Accuracy(n.ip2, n.label)\n",
        "    n.loss = L.SoftmaxWithLoss(n.ip2, n.label)\n",
        "    return n.to_proto()\n",
        "\n",
        "train_net_path = 'examples/hdf5_classification/nonlinear_auto_train.prototxt'\n",
        "with open(train_net_path, 'w') as f:\n",
        "    f.write(str(nonlinear_net('examples/hdf5_classification/data/train.txt', 10)))\n",
        "\n",
        "test_net_path = 'examples/hdf5_classification/nonlinear_auto_test.prototxt'\n",
        "with open(test_net_path, 'w') as f:\n",
        "    f.write(str(nonlinear_net('examples/hdf5_classification/data/test.txt', 10)))\n",
        "\n",
        "solver_path = 'examples/hdf5_classification/nonlinear_logreg_solver.prototxt'\n",
        "with open(solver_path, 'w') as f:\n",
        "    f.write(str(solver(train_net_path, test_net_path)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnPv7e6eh-1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%timeit\n",
        "# caffe.set_device(0)\n",
        "# caffe.set_mode_gpu()\n",
        "caffe.set_mode_cpu()\n",
        "solver = caffe.get_solver(solver_path)\n",
        "solver.solve()\n",
        "\n",
        "accuracy = 0\n",
        "batch_size = solver.test_nets[0].blobs['data'].num\n",
        "test_iters = int(len(Xt) / batch_size)\n",
        "for i in range(test_iters):\n",
        "    solver.test_nets[0].forward()\n",
        "    accuracy += solver.test_nets[0].blobs['accuracy'].data\n",
        "accuracy /= test_iters\n",
        "\n",
        "print(\"Accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9tAy2Z7h-1I",
        "colab_type": "text"
      },
      "source": [
        "Do the same through the command line interface for detailed output on the model and solving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDHFdqjlh-1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!build/tools/caffe train -solver examples/hdf5_classification/nonlinear_logreg_solver.prototxt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6zD8C9rh-1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean up (comment this out if you want to examine the hdf5_classification/data directory).\n",
        "shutil.rmtree(dirname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNTG0YJpykZZ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}